{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import datacube\n",
    "from odc import dscache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Database File\n",
    "\n",
    "You can use cli tool `slurpy` to export a set of products to a file db. But if you need more control over what goes into the cache see below.\n",
    "\n",
    "## Create new file db\n",
    "Create new database, deleting any previous files that might have existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'sample.db'\n",
    "cache = dscache.create_cache(db_name, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some datasets from Datacube\n",
    "\n",
    "We are limiting to 200 for example purposes, also there is an outstanding [issue (# 542)](https://github.com/opendatacube/datacube-core/issues/542) with `find_datasets_lazy`, it's not actually \"lazy\" the whole SQL query is processed as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(env='s2')\n",
    "dss = dc.find_datasets_lazy(product='s2a_nrt_granule', limit=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write them to file db\n",
    "\n",
    "Dataset cache provides a convenience method `.tee` that accepts dataset stream on input and generates same stream on output, but also saves datasets to the file db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = cache.tee(dss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can just iterate over all datasets doing whatever other thing you needed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "done\n",
      "CPU times: user 169 ms, sys: 5.22 ms, total: 174 ms\n",
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, ds in enumerate(dss):\n",
    "    if (i % 10) == 0:\n",
    "        print('.', end='', flush=True)\n",
    "print()\n",
    "print('done')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "`.tee` assumes that all datasets will be consumed, internally it breaks up dataset stream into transactions, it's not super clear what happens if you just stop half way through a transaction and never continue. Eventually transaction will be garbage collected and data written to disk, but in the meantime any writes will be blocked. So if you do exit early without consuming whole stream you should probably call `del dss` as soon as practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 597 Âµs, total: 135 ms\n",
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dss = dc.find_datasets_lazy(product='s2b_nrt_granule', limit=200)\n",
    "cache.bulk_save(dss) # blocks until all are written (in one single transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " {'s2a_nrt_granule': DatasetType(name='s2a_nrt_granule', id_=3),\n",
       "  's2b_nrt_granule': DatasetType(name='s2b_nrt_granule', id_=4)},\n",
       " {'eo': MetadataType(name='eo', id_=1)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.count, cache.products, cache.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin datasets into Albers Tile\n",
    "\n",
    "There is a cli tool `dstiler` that will go through all datasets in the file and bin them into various tiling regimes. Default regime is 100k side Albers tiles (same as on NCI). But there is also \"native\" for landsat scenes and \"web\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bins: 421\n",
      "\u001b[?25lSaving  [####################################]  100%\u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!dstiler sample.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from file db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " {'eo': MetadataType(name='eo', id_=None)},\n",
       " {'s2a_nrt_granule': DatasetType(name='s2a_nrt_granule', id_=None),\n",
       "  's2b_nrt_granule': DatasetType(name='s2b_nrt_granule', id_=None)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_ro = dscache.open_ro(db_name)\n",
    "cache_ro.count, cache_ro.metadata, cache_ro.products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream datasets into RAM: `.get_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datacube.model.Dataset,\n",
       " datacube.model.DatasetType,\n",
       " datacube.model.MetadataType)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dss = list(islice(cache_ro.get_all(), 10))\n",
    "type(dss[0]), type(dss[0].type), type(dss[0].type.metadata_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access individual dataset by UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset <id=0146ea8e-8462-4fca-a880-7f4a311441bf type=s2a_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2A_OPER_MSI_ARD_TL_EPAE_20190603T015756_A020605_T55KEB_N02.07/ARD-METADATA.yaml>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_ro.get('0146ea8e-8462-4fca-a880-7f4a311441bf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421, [('albers/-01_-23', 2), ('albers/-01_-24', 4), ('albers/-01_-25', 4)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from odc.dscache.tools.tiling import parse_group_name\n",
    "\n",
    "groups = cache_ro.groups()\n",
    "len(groups), groups[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('albers/-01_-24', ((-1, -24), 'albers'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_name, count = groups[1]\n",
    "group_name, parse_group_name(group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all UUIDs for a given group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('3c0b41f9-67ca-4962-83a5-68bc103e960c'),\n",
       " UUID('6d07c802-18b5-4b84-ac19-80fea9b60866'),\n",
       " UUID('9be56b1f-bc9c-4346-85ca-f4cf6fe4985a'),\n",
       " UUID('d357d1c0-7531-44ac-85b3-a5cc1e5b9eb5')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_ro.get_group(group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset documents for a given group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset <id=3c0b41f9-67ca-4962-83a5-68bc103e960c type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KHA_N02.07/ARD-METADATA.yaml>,\n",
       " Dataset <id=6d07c802-18b5-4b84-ac19-80fea9b60866 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KGA_N02.07/ARD-METADATA.yaml>,\n",
       " Dataset <id=9be56b1f-bc9c-4346-85ca-f4cf6fe4985a type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KHB_N02.07/ARD-METADATA.yaml>,\n",
       " Dataset <id=d357d1c0-7531-44ac-85b3-a5cc1e5b9eb5 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KGB_N02.07/ARD-METADATA.yaml>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cache_ro.stream_group(group_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfacing with Datacube's `GridWorkflow.load(..)`\n",
    "\n",
    "There is a helper class that can construct `datacube.mode.Tile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-1, -24), Tile<sources=<xarray.DataArray (time: 4)>\n",
       " array([(Dataset <id=9be56b1f-bc9c-4346-85ca-f4cf6fe4985a type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KHB_N02.07/ARD-METADATA.yaml>,),\n",
       "        (Dataset <id=d357d1c0-7531-44ac-85b3-a5cc1e5b9eb5 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KGB_N02.07/ARD-METADATA.yaml>,),\n",
       "        (Dataset <id=3c0b41f9-67ca-4962-83a5-68bc103e960c type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KHA_N02.07/ARD-METADATA.yaml>,),\n",
       "        (Dataset <id=6d07c802-18b5-4b84-ac19-80fea9b60866 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2019-06-03/S2B_OPER_MSI_ARD_TL_EPAE_20190603T040916_A011697_T52KGA_N02.07/ARD-METADATA.yaml>,)],\n",
       "       dtype=object)\n",
       " Coordinates:\n",
       "   * time     (time) datetime64[ns] 2019-06-03T01:23:22.587038 ... 2019-06-03T01:23:40.650139,\n",
       " \tgeobox=GeoBox(4000, 4000, Affine(25.0, 0.0, -100000.0,\n",
       "        0.0, -25.0, -2300000.0), EPSG:3577)>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from odc.dscache.tools import DcTileExtract\n",
    "\n",
    "tiles = DcTileExtract(cache_ro)\n",
    "tile_id,_ = parse_group_name(group_name)\n",
    "\n",
    "tile = tiles(tile_id)\n",
    "\n",
    "tile_id, tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then pass `tile` object to `datacube.GridWorkflow.load(..)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
